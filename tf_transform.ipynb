{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_transform.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcj8m7rOa+m1dOu9fEOrfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soerenml/colab/blob/master/tf_transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9X6mulG0U6k",
        "colab_type": "text"
      },
      "source": [
        "- Python 3 required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL012W8z0_fR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip uninstall tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klQt7uwa1AIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install tensorflow==2.1.0 tfx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGvyKPoI1CIA",
        "colab_type": "code",
        "outputId": "58ed6c8a-6347-4725-9960-7286e75d0950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Restart runtime\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEJ4HFCu4Chh",
        "colab_type": "code",
        "outputId": "aefcebcb-e499-4137-99f5-ae8be2ac14ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip uninstall tensorflow-transform\n",
        "!pip uninstall apache-beam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-transform-0.21.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_transform-0.21.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_transform/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-transform-0.21.0\n",
            "Uninstalling apache-beam-2.17.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/apache_beam-2.17.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/apache_beam/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled apache-beam-2.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgMdHKRw0IXj",
        "colab_type": "code",
        "outputId": "4a8a67d6-2d11-4f65-c6ec-a11e2181992b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# ToDo: Change the versioning!\n",
        "# Jumo back and forth while installing.\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "print(\"Installing dependencies for Colab environment\")\n",
        "!pip install -q -Uq grpcio==1.26.0\n",
        "\n",
        "print('Installing Apache Beam')\n",
        "!pip install -Uq apache_beam==2.17.0\n",
        "import apache_beam as beam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies for Colab environment\n",
            "Installing Apache Beam\n",
            "\u001b[31mERROR: tfx 0.21.0 requires tensorflow-transform<0.22,>=0.21, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-data-validation 0.21.1 requires tensorflow-transform<0.22,>=0.21, which is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFP_sn7-2c9U",
        "colab_type": "code",
        "outputId": "483b82f9-e81c-405b-c0e5-f6ea38a42e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Installing TensorFlow Transform')\n",
        "!pip install -Uq tensorflow-transform==0.21\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import apache_beam.io.iobase\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import dataset_schema"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing TensorFlow Transform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg2p6EpV5n0z",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OGTi90n0fP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "  \"\"\"\n",
        "  Create directory and extract the data (housing.csv)\n",
        "  \"\"\"\n",
        "  os.makedirs(housing_path, exist_ok=True)\n",
        "  tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "  urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "  housing_tgz = tarfile.open(tgz_path)\n",
        "  housing_tgz.extractall(path=housing_path)\n",
        "  housing_tgz.close()\n",
        "\n",
        "fetch_housing_data()\n",
        "\n",
        "# Path to the local CSV file\n",
        "data_path=\"datasets/housing/housing.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujxQUcgN6BFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORICAL_FEATURE_KEYS = ['ocean_proximity']\n",
        "NUMERIC_FEATURE_KEYS = [\n",
        "                        \"longitude\", \"latitude\", \"housing_median_age\",\t\n",
        "                        \"total_rooms\", \"total_bedrooms\", \"population\",\t\n",
        "                        \"households\", \"median_income\",\n",
        "                        ]\n",
        "\n",
        "LABEL_KEY = 'median_house_value'\n",
        "\n",
        "\n",
        "# Define the feature specifications\n",
        "RAW_DATA_FEATURE_SPEC = dict(\n",
        "    [(name, tf.io.FixedLenFeature([], tf.string))\n",
        "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
        "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "     for name in NUMERIC_FEATURE_KEYS] +\n",
        "    [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))]\n",
        ")\n",
        "\n",
        "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
        "    dataset_schema.from_feature_spec(RAW_DATA_FEATURE_SPEC))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpjq9JJ97Vs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing = os.getenv(\"WEB_TEST_BROWSER\", False)\n",
        "if testing:\n",
        "  TRAIN_NUM_EPOCHS = 1\n",
        "  NUM_TRAIN_INSTANCES = 1\n",
        "  TRAIN_BATCH_SIZE = 1\n",
        "  NUM_TEST_INSTANCES = 1\n",
        "else:\n",
        "  TRAIN_NUM_EPOCHS = 16\n",
        "  NUM_TRAIN_INSTANCES = 32561\n",
        "  TRAIN_BATCH_SIZE = 128\n",
        "  NUM_TEST_INSTANCES = 16281\n",
        "\n",
        "# Names of temp files\n",
        "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
        "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
        "EXPORTED_MODEL_DIR = 'exported_model_dir'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5NGk7m-7icf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MapAndFilterErrors(beam.PTransform):\n",
        "  \"\"\"Like beam.Map but filters out errors in the map_fn.\"\"\"\n",
        "\n",
        "  class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
        "    \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
        "\n",
        "    def __init__(self, fn):\n",
        "      self._fn = fn\n",
        "      # Create a counter to measure number of bad elements.\n",
        "      self._bad_elements_counter = beam.metrics.Metrics.counter(\n",
        "          'census_example', 'bad_elements')\n",
        "\n",
        "    def process(self, element):\n",
        "      try:\n",
        "        yield self._fn(element)\n",
        "      except Exception:  # pylint: disable=broad-except\n",
        "        # Catch any exception the above call.\n",
        "        self._bad_elements_counter.inc(1)\n",
        "\n",
        "  def __init__(self, fn):\n",
        "    self._fn = fn\n",
        "\n",
        "  def expand(self, pcoll):\n",
        "    return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgDAIQ9r7mNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "  # Since we are modifying some features and leaving others unchanged, we\n",
        "  # start by setting `outputs` to a copy of `inputs.\n",
        "  outputs = inputs.copy()\n",
        "\n",
        "  # Scale numeric columns to have range [0, 1].\n",
        "  for key in NUMERIC_FEATURE_KEYS:\n",
        "    outputs[key] = tft.scale_to_0_1(outputs[key])\n",
        "\n",
        "    # For all categorical columns except the label column, we generate a\n",
        "    # vocabulary but do not modify the feature.  This vocabulary is instead\n",
        "    # used in the trainer, by means of a feature column, to convert the feature\n",
        "    # from a string to an integer id.\n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    tft.vocabulary(inputs[key], vocab_filename=key)\n",
        "\n",
        "  return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx_L6k0c8BdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_data(train_data_file, test_data_file, working_dir):\n",
        "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "  Args:\n",
        "    train_data_file: File containing training data\n",
        "    test_data_file: File containing test data\n",
        "    working_dir: Directory to write transformed data and metadata to\n",
        "  \"\"\"\n",
        "\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "      # Create a coder to read the census data with the schema.  To do this we\n",
        "      # need to list all columns in order since the schema doesn't specify the\n",
        "      # order of columns in the csv.\n",
        "      ordered_columns = [\"longitude\",\t\"latitude\",\t\"housing_median_age\",\t\n",
        "                         \"total_rooms\",\t\"total_bedrooms\",\t\"population\",\t\n",
        "                         \"households\",\t\"median_income\",\t\"median_house_value\",\n",
        "                         \"ocean_proximity\"]\n",
        "                         \n",
        "      converter = tft.coders.CsvCoder(ordered_columns, RAW_DATA_METADATA.schema)\n",
        "\n",
        "      # Read in raw data and convert using CSV converter.  Note that we apply\n",
        "      # some Beam transformations here, which will not be encoded in the TF\n",
        "      # graph since we don't do them from within tf.Transform's methods\n",
        "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
        "      # to get data into a format that the CSV converter can read, in particular\n",
        "      # removing spaces after commas.\n",
        "      #\n",
        "      # We use MapAndFilterErrors instead of Map to filter out decode errors in\n",
        "      # convert.decode which should only occur for the trailing blank line.\n",
        "      raw_data = (\n",
        "          pipeline\n",
        "          | 'ReadTrainData' >> beam.io.ReadFromText(train_data_file)\n",
        "          | 'FixCommasTrainData' >> beam.Map(\n",
        "              lambda line: line.replace(', ', ','))\n",
        "          | 'DecodeTrainData' >> MapAndFilterErrors(converter.decode))\n",
        "\n",
        "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "      # the schema to read the CSV data, but we also need it to interpret\n",
        "      # raw_data.\n",
        "      raw_dataset = (raw_data, RAW_DATA_METADATA)\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n",
        "      transformed_data, transformed_metadata = transformed_dataset\n",
        "      transformed_data_coder = tft.coders.ExampleProtoCoder(\n",
        "          transformed_metadata.schema)\n",
        "\n",
        "      _ = (\n",
        "          transformed_data\n",
        "          | 'EncodeTrainData' >> beam.Map(transformed_data_coder.encode)\n",
        "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
        "\n",
        "      # Now apply transform function to test data.  In this case we remove the\n",
        "      # trailing period at the end of each line, and also ignore the header line\n",
        "      # that is present in the test data file.\n",
        "      raw_test_data = (\n",
        "          pipeline\n",
        "          | 'ReadTestData' >> beam.io.ReadFromText(test_data_file,\n",
        "                                                   skip_header_lines=1)\n",
        "          | 'FixCommasTestData' >> beam.Map(\n",
        "              lambda line: line.replace(', ', ','))\n",
        "          | 'RemoveTrailingPeriodsTestData' >> beam.Map(lambda line: line[:-1])\n",
        "          | 'DecodeTestData' >> MapAndFilterErrors(converter.decode))\n",
        "\n",
        "      raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
        "\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn) | tft_beam.TransformDataset())\n",
        "      # Don't need transformed data schema, it's the same as before.\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'EncodeTestData' >> beam.Map(transformed_data_coder.encode)\n",
        "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
        "\n",
        "      # Will write a SavedModel and metadata to working_dir, which can then\n",
        "      # be read by the tft.TFTransformOutput class.\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGiejimP8kYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _make_training_input_fn(tf_transform_output, transformed_examples,\n",
        "                            batch_size):\n",
        "  \"\"\"Creates an input function reading from transformed data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    transformed_examples: Base filename of examples.\n",
        "    batch_size: Batch size.\n",
        "\n",
        "  Returns:\n",
        "    The input function for training or eval.\n",
        "  \"\"\"\n",
        "  def input_fn():\n",
        "    \"\"\"Input function for training and eval.\"\"\"\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=transformed_examples,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        shuffle=True)\n",
        "\n",
        "    transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n",
        "        dataset).get_next()\n",
        "\n",
        "    # Extract features and label from the transformed tensors.\n",
        "    transformed_labels = transformed_features.pop(LABEL_KEY)\n",
        "\n",
        "    return transformed_features, transformed_labels\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3dIqW-q8nhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _make_serving_input_fn(tf_transform_output):\n",
        "  \"\"\"Creates an input function reading from raw data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "\n",
        "  Returns:\n",
        "    The serving input function.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n",
        "  # Remove label since it is not available during serving.\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  def serving_input_fn():\n",
        "    \"\"\"Input function for serving.\"\"\"\n",
        "    # Get raw features by generating the basic serving input_fn and calling it.\n",
        "    # Here we generate an input_fn that expects a parsed Example proto to be fed\n",
        "    # to the model at serving time.  See also\n",
        "    # tf.estimator.export.build_raw_serving_input_receiver_fn.\n",
        "    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "        raw_feature_spec, default_batch_size=None)\n",
        "    serving_input_receiver = raw_input_fn()\n",
        "\n",
        "    # Apply the transform function that was used to generate the materialized\n",
        "    # data.\n",
        "    raw_features = serving_input_receiver.features\n",
        "    transformed_features = tf_transform_output.transform_raw_features(\n",
        "        raw_features)\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(\n",
        "        transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "  return serving_input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DPNi-me8rHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_columns(tf_transform_output):\n",
        "  \"\"\"Returns the FeatureColumns for the model.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A `TFTransformOutput` object.\n",
        "\n",
        "  Returns:\n",
        "    A list of FeatureColumns.\n",
        "  \"\"\"\n",
        "  # Wrap scalars as real valued columns.\n",
        "  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n",
        "                         for key in NUMERIC_FEATURE_KEYS]\n",
        "\n",
        "  # Wrap categorical columns.\n",
        "  one_hot_columns = [\n",
        "      tf.feature_column.categorical_column_with_vocabulary_file(\n",
        "          key=key,\n",
        "          vocabulary_file=tf_transform_output.vocabulary_file_by_name(\n",
        "              vocab_filename=key))\n",
        "      for key in CATEGORICAL_FEATURE_KEYS]\n",
        "\n",
        "  return real_valued_columns + one_hot_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuwyRujV8uFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n",
        "                       num_test_instances=NUM_TEST_INSTANCES):\n",
        "  \"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  Args:\n",
        "    working_dir: Directory to read transformed data and metadata from and to\n",
        "        write exported model to.\n",
        "    num_train_instances: Number of instances in train set\n",
        "    num_test_instances: Number of instances in test set\n",
        "\n",
        "  Returns:\n",
        "    The results from the estimator's 'evaluate' method\n",
        "  \"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "\n",
        "  run_config = tf.estimator.RunConfig()\n",
        "\n",
        "  estimator = tf.estimator.LinearClassifier(\n",
        "      feature_columns=get_feature_columns(tf_transform_output),\n",
        "      config=run_config,\n",
        "      loss_reduction=tf.losses.Reduction.SUM)\n",
        "\n",
        "  # Fit the model using the default optimizer.\n",
        "  train_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
        "      batch_size=TRAIN_BATCH_SIZE)\n",
        "  estimator.train(\n",
        "      input_fn=train_input_fn,\n",
        "      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / TRAIN_BATCH_SIZE)\n",
        "\n",
        "  # Evaluate model on test dataset.\n",
        "  eval_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
        "      batch_size=1)\n",
        "\n",
        "  # Export the model.\n",
        "  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n",
        "  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
        "  estimator.export_saved_model(exported_model_dir, serving_input_fn)\n",
        "\n",
        "  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KK4zqLw8zYq",
        "colab_type": "code",
        "outputId": "e893d0dc-80cc-47d6-98ec-0104c06693c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import shutil\n",
        "temp = tempfile.gettempdir()\n",
        "train = os.path.join(data_path)\n",
        "test = os.path.join(data_path)\n",
        "print(temp)\n",
        "print(train)\n",
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prepro\n",
            "datasets/housing/housing.csv\n",
            "datasets/housing/housing.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5J6E9Zt9Xha",
        "colab_type": "code",
        "outputId": "a9dc9ec2-92de-44a0-9a52-cdd0a522273d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "transform_data(train, test, prepro)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-eea3a9e24412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-4731ef558039>\u001b[0m in \u001b[0;36mtransform_data\u001b[0;34m(train_data_file, test_data_file, working_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtft_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0;31m# Create a coder to read the census data with the schema.  To do this we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;31m# need to list all columns in order since the schema doesn't specify the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tempfile.py\u001b[0m in \u001b[0;36mmkdtemp\u001b[0;34m(suffix, prefix, dir)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0o700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mcontinue\u001b[0m    \u001b[0;31m# try again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/var/tmp/tmpi53u_g1g'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGRjqijLEIaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = train_and_evaluate(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}